{"cells":[{"cell_type":"markdown","metadata":{"id":"TCxxWBZioi0N"},"source":["# REINFORCE\n","\n","> Implement the REINFORCE algorithm.\n"]},{"cell_type":"markdown","source":["#### Install the libraries"],"metadata":{"id":"n0Q3kAnpoCp3"}},{"cell_type":"code","source":["!apt-get install -y xvfb\n","\n","!pip install \\\n","  pygame \\\n","  gym==0.23.1 \\\n","  pytorch-lightning==1.6 \\\n","  pyvirtualdisplay"],"metadata":{"id":"oyQ7ov4M8pYR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZOSJl-X7zvs4"},"source":["#### Setup virtual display\n","> Its for allowing us to render the enviroment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-Z6takfzqGk"},"outputs":[],"source":["from pyvirtualdisplay import Display\n","Display(visible=False, size=(1400, 900)).start()"]},{"cell_type":"markdown","metadata":{"id":"Cz8DLleGz_TF"},"source":["#### Import the necessary code libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cP5t6U7-nYoc"},"outputs":[],"source":["import copy\n","import torch\n","import random\n","import gym\n","import matplotlib\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch.nn.functional as F\n","\n","from collections import deque, namedtuple\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import IterableDataset\n","from torch.optim import AdamW\n","\n","from pytorch_lightning import LightningModule, Trainer\n","\n","from gym.wrappers import RecordVideo, RecordEpisodeStatistics, \\\n","  NormalizeObservation, NormalizeReward\n","\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","num_gpus = torch.cuda.device_count()"]},{"cell_type":"markdown","source":["#### Utils functions"],"metadata":{"id":"juDbWgaypes9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_IrPlU1wwPx"},"outputs":[],"source":["def plot_policy(policy):\n","  \"\"\"\n","  Plotting the policy in a hitmap.\n","  ------------------------------\n","   computes the probability of taking a \"left\" action for each\n","   state according to the policy,\n","   and then visualizes these probabilities using a heatmap.\n","  \"\"\"\n","\n","  # 1. Grid Creation\n","  # Initial arrays representing different aspects of the\n","  # state space (position, velocity, angle, and angular velocity).\n","  pos = np.linspace(-4.8, 4.8, 100)\n","  vel = np.random.random(size=(10000, 1)) * 0.1\n","  ang = np.linspace(-0.418, 0.418, 100)\n","  ang_vel = np.random.random(size=(10000, 1)) * 0.1\n","\n","  # 2. State Grid Construction:\n","  # - The state grid is constructed by stacking the meshgrid arrays and the\n","  #   velocity/angular velocity arrays along the last axis.\n","  # - The resulting grid array represents combinations of\n","  #   position, angle, velocity, and angular velocity.\n","  g1, g2 = np.meshgrid(pos, ang)\n","  grid = np.stack((g1,g2), axis=-1)\n","  grid = grid.reshape(-1, 2)\n","  grid = np.hstack((grid, vel, ang_vel))\n","\n","  # 3. Policy Evaluation:\n","  # The policy function (policy) is applied to the state grid.\n","  # The result is a tensor of probabilities for each action\n","  # (e.g., moving left or right).\n","  probs = policy(grid).detach().numpy()\n","  probs_left = probs[:, 0]\n","\n","  # 4. Data Reshaping:\n","  # The probabilities for the \"left\" action (probs_left) are reshaped\n","  # to match the shape of the position and angle grid.\n","  probs_left = probs_left.reshape(100, 100)\n","  probs_left = np.flip(probs_left, axis=1)\n","\n","  # 5. Plotting\n","  plt.figure(figsize=(8, 8))\n","  plt.imshow(probs_left, cmap='coolwarm')\n","  plt.colorbar()\n","  plt.clim(0, 1)\n","  plt.title(\"P(left | s)\", size=20)\n","  plt.xlabel(\"Cart Position\", size=14)\n","  plt.ylabel(\"Pole angle\", size=14)\n","  plt.xticks(ticks=[0, 50, 100], labels=['-4.8', '0', '4.8'])\n","  plt.yticks(ticks=[100, 50, 0], labels=['-0.418', '0', '0.418'])\n"]},{"cell_type":"code","source":["def test_env(env_name, policy, obs_rms):\n","  \"\"\"\n","  Test a reinforcement learning policy on a specified environment.\n","\n","\n","  Parameters:\n","  ----------\n","  - `env_name`: the name of the env.\n","  - `policy`: the policy of the agent (givan a state, give me an action...)\n","  - `obs_rms`: the observation root mean square value of the environment.\n","               This is a precomputed value used for normalization.\n","\n","  \"\"\"\n","  env = gym.make(env_name) # create the env\n","  env = RecordVideo(env, 'videos', episode_trigger=lambda e: True) # record the video\n","  env = NormalizeObservation(env) # normalize the observation in the env\n","  env.obs_rms = obs_rms # set the observation root mean square value of the environment\n","\n","  for episode in range(10): # loop over `n` episodes\n","    done = False\n","    obs = env.reset() # get the initial state (first observation)\n","    while not done: # while it not done, play in the game\n","      action = policy(obs).multinomial(1).cpu().item() # given an observation, produce an action based on the rl policy\n","      obs, _, done, _ = env.step(action) # perform the action in the env\n","  del env"],"metadata":{"id":"z7FBxlMtr37m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def display_video(episode=0):\n","  \"\"\"\n","  Display a vidoe in HTML format.\n","  \"\"\"\n","  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n","  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n","  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"],"metadata":{"id":"KFzGhrmor6r1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bnk0wSWj0hAz"},"source":["#### Create the policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9a0b9cdnYtT"},"outputs":[],"source":["class GradientPolicy(nn.Module):\n","  \"\"\"\n","  A policy gradient class\n","  -----------------------\n","  In DRL, the policy is a neural network.\n","  So the neural network will get an action, and predict the\n","  best policy that can be.\n","\n","  it's designed to map input states to probability distributions over\n","  a set of discrete actions.\n","\n","  by this policy we are going to solve the cart pole problem.\n","  \"\"\"\n","\n","  def __init__(self, in_features, n_actions, hidden_size=128):\n","    super().__init__()\n","    self.fc1 = nn.Linear(in_features, hidden_size) # input: states\n","    self.fc2 = nn.Linear(hidden_size, hidden_size)\n","    self.fc3 = nn.Linear(hidden_size, n_actions) # output: actions\n","\n","  def forward(self, x):\n","    x = torch.tensor(x).float().to(device)\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","    # apply softmax for get probability distributions of actions.\n","    # e.g, the probability distribution to take the actions left and rigth.\n","    # -> ([0.44,0.56]) -> ([probability of L, probability of R])\n","    x = F.softmax(self.fc3(x), dim=-1)\n","    return x"]},{"cell_type":"markdown","source":["#### Plot the untrained policy\n","> Its literally the initial (random) weigths of the `GradientPolicy` class."],"metadata":{"id":"sHcFutvhJ9Ot"}},{"cell_type":"code","source":["policy = GradientPolicy(4, 2) # input,output (state/action)\n","grid = plot_policy(policy)\n","\n","# Interpretation:\n","# we plot the probability to take action `left` give state `s`.\n","# - on the y-axis you have the angle of the pole.\n","# - on the x-axis you have the position (location) of the cart\n","#   (the pole in on the cart).\n","# - the color-map shows us the probability of take the action `left`.\n","#   - if the probability is 1, its 100% that action `left`.\n","#   - if the probability is 0, its 100% that action `right`.\n","\n","# you can see that without training out nn choose actions very randomaly."],"metadata":{"id":"8gsTHMaNJ8ZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grid"],"metadata":{"id":"RavjH1SEL_4g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0yvDC9qF0oPr"},"source":["#### Create the environment (several copies of env)\n","> Creating multiple environments in parallel can be useful for training reinforcement learning agents more efficiently, especially when using vectorized environments."]},{"cell_type":"code","source":["# Create `num_envs` enviroments (its for perellal)\n","env = gym.vector.make(\"CartPole-v1\", num_envs=2)"],"metadata":{"id":"Ry7ECCTdWB5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# as you can see, we have two rows.\n","# the first row -> is the initial state of the first env\n","# the second row -> is the initial state of the second env\n","env.reset()\n","\n","# states:\n","# Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity"],"metadata":{"id":"0Pbxh84fYK_b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# see the observation space:\n","# its show us the minimum value,\n","# and the maximum values that can be in the states.\n","print(env.observation_space)\n","print()\n","\n","# the action spcace: 2 in env_1, 2 in env_2.\n","print(env.action_space)"],"metadata":{"id":"kjkGYTxUYLB0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lets test the interaction with the envs:\n","\n","# create two actions for the two envs:\n","# take left in env_1_ take laft in env_2.\n","actions = np.array([0, 0])\n","\n","# let's perdorm the actions in the environments\n","next_obs, rewards, dones, infos = env.step(actions)"],"metadata":{"id":"jD_2PVe_gS2D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# this is the next state in the enviroments:\n","next_obs"],"metadata":{"id":"eh7UG1_LgT8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the rewards:\n","rewards"],"metadata":{"id":"7FGmxE1Gh12i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# if the game is done:\n","dones"],"metadata":{"id":"nNixlpRxh15T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# additional information\n","infos"],"metadata":{"id":"xzvNiamFh2p3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_env(env_name, num_envs):\n","  \"\"\"\n","  Function to create multiple environments\n","  to be solved with by `torch lightning` library.\n","\n","  Parameters:\n","  ------------\n","  - `env_name`: the name of the environment.\n","  - `num_envs`: how many environment to create.\n","  \"\"\"\n","  # create the environment(s)\n","  env = gym.vector.make(env_name, num_envs=num_envs)\n","\n","  # Wrapper the env: for saving the executions statistics in an array that\n","  # we can access the outcomes of the previous episodes.\n","  env = RecordEpisodeStatistics(env)\n","\n","  # Wrapper the env: normalize the observation\n","  # (neural network loves normalize data)\n","  env = NormalizeObservation(env)\n","\n","  # Wrapper: normalize the rewards also.\n","  env = NormalizeReward(env)\n","\n","  return env"],"metadata":{"id":"jUYWliZXYLEJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Create the dataset\n"],"metadata":{"id":"A5L81w8HxVn4"}},{"cell_type":"code","source":["class RLDataset(IterableDataset):\n","  \"\"\"\n","  a custom iterable dataset for reinforcement learning.\n","\n","  - It generates batches of training data for training a policy.\n","  - The dataset consists of transitions,\n","    where each transition includes an observation,\n","    an action, and the return (cumulative discounted rewards).\n","\n","  Parameters:\n","  -----------\n","  - `env`: The environment for which the dataset is generated.\n","  - `policy`: The policy used to generate actions in the environment.\n","  - `steps_per_epoch`: The number of transitions to generate for each epoch.\n","  - `gamma`: The discount factor for calculating the cumulative discounted rewards.\n","  - `obs`: The current observation, initialized by resetting the environment.\n","  \"\"\"\n","\n","  # Initialization\n","  def __init__(self, env, policy, steps_per_epoch, gamma):\n","    self.env = env\n","    self.policy = policy\n","    self.steps_per_epoch = steps_per_epoch\n","    self.gamma = gamma\n","    self.obs = env.reset()\n","\n","  # Iteration method\n","  @torch.no_grad()\n","  def __iter__(self): # an iterator for the dataset.\n","    \"\"\"\n","    Iteration method\n","    defines how to return the elements of the dataset in sequence.\n","    \"\"\"\n","\n","    # list where we store the transition where we interactive with the env.\n","    transitions = []\n","\n","    # Transition Generation (interact with the env, save the transitions.)\n","    for step in range(self.steps_per_epoch):\n","      action = self.policy(self.obs) # choose an action\n","      action = action.multinomial(1).cpu().numpy() # prepare the action to be pass to the env\n","      next_obs, reward, done, info = self.env.step(action.flatten()) # perform the action (flatten(): [[a],[a]] -> [a,a])\n","      transitions.append((self.obs, action, reward, done)) # append the trandistion\n","      self.obs = next_obs # update the current observation\n","\n","    ## create a tensores of all the observation, actios, rewards, dones.\n","    # zip(*transitions) -> unpacking the list of transitions (convert to tuple (*)).\n","    # after we have tuples (obs, action, reward, done),\n","    # stack -> stack arrays along a new axis. (each variable have array.)\n","    # obs_b, action_b,... -> are all arrays.\n","    obs_b, action_b, reward_b, done_b = map(np.stack, zip(*transitions))\n","\n","    # our algorithms doesn't learn from rewards,\n","    # but based on the return! so we need to sume all the rewards.\n","    # its mean that for each time step we need to compute the\n","    # discountet sum of the rewards.\n","\n","    # initial arrays (for each env) of zeros (for store the return.):\n","    # - this is for store the running sum of rewards\n","    # - This running sum is then used to calculate the return for each time step.\n","    running_return = np.zeros(self.env.num_envs, dtype=np.float32)\n","    # - this is for store the final returns for each time step,\n","    #   taking into account the cumulative discounted sum of rewards.\n","    # batch of returns\n","    return_b = np.zeros_like(reward_b)\n","\n","    ## calculating the return for each time step in reverse orde\n","    for row in range(self.steps_per_epoch - 1, -1, -1): # looping in Reverse\n","      # updates the running return for the current time step (row).\n","      # its the reward times the previes, rcursivily (cumulative discounted sum of rewards,)\n","      running_return = reward_b[row] + (1 - done_b[row]) * self.gamma * running_return\n","      # store the calculated running return for the current time step\n","      return_b[row] = running_return\n","      # > Now we have the array of returns that the algorithm needs to\n","      #   be able to train our policy...\n","\n","\n","    # make sure that all the batches have the rigth shape (for shuffles that)\n","    # compute the num of samples\n","    num_samples = self.env.num_envs * self.steps_per_epoch\n","    obs_b = obs_b.reshape(num_samples, -1) # -> 1D\n","    action_b = action_b.reshape(num_samples, -1) # -> 1D\n","    return_b = return_b.reshape(num_samples, -1) # -> 1D\n","\n","    # create a list of all the indexes as the number of samples\n","    idx = list(range(num_samples))\n","    # shuffle the observations\n","    random.shuffle(idx)\n","\n","    # loop for each index (in the shuffeled order)\n","    for i in idx:\n","      # return the observation, action, and return\n","      yield obs_b[i], action_b[i], return_b[i]\n","\n","\n","\n","# Now our dataset is ready to be used every time that our algorithm\n","# has to load the observations into batches to undergo training,\n","# it will call this __iter__ method and it will get number of\n","# observations in a random order."],"metadata":{"id":"92TmeOeqetcz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sgXi6A4Z1p75"},"source":["#### Create the REINFORCE algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOmxUJ1vnY5d"},"outputs":[],"source":["class Reinforce(LightningModule):\n","  \"\"\"\n","  PyTorch Lightning module for training a policy using the REINFORCE algorithm.\n","\n","  Parameters:\n","  -----------\n","   - `env_name`: the name of the enviroment\n","   - `num_envs`: number of environments\n","   - `samples_per_epoch`: number of samples (observations) to collect in each epoch\n","   - `batch_size`: The size of the batches used during training. It determines how many samples will be processed together in each iteration.\n","   - `hidden_size`: number of hidden layers in the policy network.\n","   - `policy_lr` The learning rate used for optimizing the policy network.\n","   - `gamma`: discount factor used in the computation of the cumulative discounted sum of rewards.\n","   - `entropy_coef`: The coefficient for the entropy regularization term in the loss function. It controls the amount of entropy regularization applied to the policy.\n","   - `optim`: the optimizer for update the policy network\n","  \"\"\"\n","  def __init__(self, env_name, num_envs=8, samples_per_epoch=1000,\n","               batch_size=1024, hidden_size=64, policy_lr=0.001,\n","               gamma=0.99, entropy_coef=0.001, optim=AdamW):\n","\n","    super().__init__()\n","\n","    self.env = create_env(env_name, num_envs=num_envs) # create enviroments\n","\n","    # get the shape of the observations (Input shape od policy ANN)\n","    # how many features each of env have ?\n","    obs_size = self.env.single_observation_space.shape[0]\n","    # get the number of actions (Output shape of policy ANN)\n","    # number of avaulable action (action space size.)\n","    n_actions = self.env.single_action_space.n\n","\n","    # Create an instance of the ANN policy,\n","    # and define the input & output shapes and the numner of hidden units state.\n","    self.policy = GradientPolicy(obs_size, n_actions, hidden_size)\n","\n","    # create the data set,\n","    # send the environments, the policy ANN, an so forth.\n","    self.dataset = RLDataset(self.env, self.policy, samples_per_epoch, gamma)\n","\n","    # save each of the parameters of the class so\n","    # we'll can reference them any where from the code\n","    self.save_hyperparameters() # just a convince method..\n","\n","  # Configure optimizers.\n","  def configure_optimizers(self):\n","    \"\"\"\n","    Create Adam optimizer with the parameter of our policy.\n","    This method returns the optimizer that will be used to update the\n","    parameters of the policy network during the training process.\n","\n","    - self.policy.parameters():\n","      - provides the parameters (weights and biases) of the policy network.\n","        The optimizer will update these parameters during the\n","        optimization process.\n","\n","    \"\"\"\n","    return self.hparams.optim(self.policy.parameters(),\n","                              lr=self.hparams.policy_lr)\n","\n","\n","  def train_dataloader(self):\n","    \"\"\"\n","    DataLoader that will be used to iterate over batches of\n","    training data during the training process.\n","\n","    - self.dataset: This part accesses the dataset used for training\n","    - batch_size: how many observations we want to group together before passing them to the training step\n","    \"\"\"\n","    return DataLoader(dataset=self.dataset, batch_size=self.hparams.batch_size)\n","\n","  # Training step.\n","  def training_step(self, batch, batch_idx):\n","    \"\"\"\n","    Execute a training step.\n","\n","    Parameters:\n","    ----------\n","    - `batch`: the batch that contain the observations.\n","    - `batch_idx`: the index of this batch.\n","    \"\"\"\n","\n","    # extract the observation, actions, returns from the batch\n","    obs, actions, returns = batch\n","\n","    # given the state (obs), return the actions probabilities\n","    # (based on the policy ANN)\n","    probs = self.policy(obs)\n","\n","    ## entropy part\n","    # compute the log of the action probability (that came from the ANN policy)\n","    log_probs = torch.log(probs + 1e-6)\n","    # take the logarithem (base 1) of the action that we took in that state\n","    action_log_prob = log_probs.gather(1, actions)\n","\n","    # compute the entropy of the probability distribution\n","    entropy = -torch.sum(probs * log_probs, dim=-1, keepdim=True)\n","\n","    # what that we want to minimize\n","    pg_loss = -action_log_prob * returns\n","    # compute the total loss with discount to the entropy (e.g, include 0.8 from the entropy)\n","    loss = (pg_loss - self.hparams.entropy_coef * entropy).mean()\n","\n","    # save in the log (for plot later)\n","    self.log(\"episode/PG Loss\", pg_loss.mean())\n","    self.log(\"episode/Entropy\", entropy.mean())\n","\n","    # return the total loss\n","    return loss\n","\n","  def training_epoch_end(self, training_step_outputs):\n","    \"\"\"\n","    for reach the return of each epicode.\n","    \"\"\"\n","    self.log(\"episode/Return\", self.env.return_queue[-1])"]},{"cell_type":"markdown","metadata":{"id":"6mm9P0sX1wAA"},"source":["#### Purge logs and run the visualization tool (Tensorboard)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfGQdpn0nY99"},"outputs":[],"source":["!rm -r /content/lightning_logs/\n","!rm -r /content/videos/\n","%load_ext tensorboard\n","%tensorboard --logdir /content/lightning_logs/"]},{"cell_type":"markdown","metadata":{"id":"G8GdIwla1wrW"},"source":["#### Train the policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ig8c_RM8nZLN"},"outputs":[],"source":["algo = Reinforce('CartPole-v1')\n","\n","# train the agent using the `Trainer` class\n","trainer = Trainer(\n","  gpus=num_gpus, # number of computation GPU units\n","  max_epochs=100,\n","  log_every_n_steps=1 # update the log each 1 step.\n",")\n","\n","# run the training\n","trainer.fit(algo)"]},{"cell_type":"markdown","metadata":{"id":"jD3x39w71xWR"},"source":["#### Check the resulting policy"]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","# test the agant!\n","test_env('CartPole-v1', algo.policy, algo.env.obs_rms)"],"metadata":{"id":"YJSqX0Xj4IW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PleQkLR-yNM"},"outputs":[],"source":["display_video(episode=1)"]},{"cell_type":"markdown","source":["#### Plot the trained policy"],"metadata":{"id":"eWQ2YY7gJyLw"}},{"cell_type":"code","source":["plot_policy(algo.policy)"],"metadata":{"id":"pPV_0MwT_NvB"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}